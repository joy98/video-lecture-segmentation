{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video_lecture_segmentation_using_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJGOiWtcDIB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4353f12-651f-4993-dc22-ca74469e301b"
      },
      "source": [
        "!git clone https://github.com/joy98/video-lecture-segmentation.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'video-lecture-segmentation'...\n",
            "remote: Enumerating objects: 624, done.\u001b[K\n",
            "remote: Total 624 (delta 0), reused 0 (delta 0), pack-reused 624\u001b[K\n",
            "Receiving objects: 100% (624/624), 14.69 MiB | 23.91 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEpMQITTNiev",
        "outputId": "6399afd3-62a3-49a7-c2ad-cba4dbd10005"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7pn0vsNiu0"
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from nltk import*"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul64_2ZJNiyf"
      },
      "source": [
        "'''\n",
        "    NOUN PHRASE EXTRACTION\n",
        "'''\n",
        "freqW=stopwords.words('english')      #list containing stop words\n",
        "\n",
        "# Defining a grammar & Parser\n",
        "NP = \"NP: {<NN.*><VB.?>+<NN.*>}\"\n",
        "chunker = RegexpParser(NP)\n",
        "\n",
        "def get_continuous_chunks(text, chunk_func=ne_chunk):\n",
        "    ptxt=''\n",
        "    for w in text.split(' '):  #removing stop words\n",
        "        if w not in freqW:\n",
        "            ptxt+=w+' '\n",
        "    text=ptxt.strip()\n",
        "    chunked = chunk_func(pos_tag(word_tokenize(text)))\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for subtree in chunked:\n",
        "        if type(subtree) == Tree:\n",
        "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
        "        elif current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return continuous_chunk"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q5QoLT6Ni14"
      },
      "source": [
        "path=r\"/content/video-lecture-segmentation/ALV_srt\"\n",
        "tmp=[]\n",
        "for i in range(1,100):\n",
        "    tmp.append(str('%04d'%i))\n",
        "files=[open(path+'/'+i+'.srt',encoding=\"utf8\") for i in tmp]\n",
        "files=[i.read() for i in files]\n",
        "files=[word_tokenize(i) for i in files]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFKL9bkxNi5Q"
      },
      "source": [
        "#regex for words and time in the text\n",
        "regex1 = re.compile(\"([a-zA-Z]+)\")\n",
        "regex2 = re.compile(\"\\d\\d\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8aG7sIlNi8x"
      },
      "source": [
        "words_list=[]   #word_list contains the words from the input text\n",
        "words_time=[]   #words_time contains the time corresponding to each words from words_list\n",
        "\n",
        "start_time,end_time=None,None\n",
        "for file in files:\n",
        "    tmp=[]\n",
        "    ptm=[]\n",
        "    for word in file:\n",
        "        if regex1.match(word):\n",
        "            k=regex1.match(word).span()\n",
        "            tmp.append(word[k[0]:k[1]])\n",
        "            ptm.append(start_time)\n",
        "        elif regex2.match(word):\n",
        "            if start_time==None and end_time==None:\n",
        "                start_time=word\n",
        "            elif end_time==None:\n",
        "                end_time=word\n",
        "            else:\n",
        "                start_time=word\n",
        "                end_time=None\n",
        "    words_list.append(tmp)\n",
        "    words_time.append(ptm)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRV0OJChNjF_"
      },
      "source": [
        "# processed_word=[]\n",
        "# for ws in words_list:\n",
        "#     tmp=[]\n",
        "#     for word in ws:\n",
        "#         #if(word not in freqW):\n",
        "#         tmp.append(word.lower())\n",
        "#     processed_word.append(tmp)\n",
        "# words_list=processed_word"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hoot2zMLNjJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8ecec6-1066-45fa-9c89-3942f5161f73"
      },
      "source": [
        "pip install -q tensorflow-text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 28.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST4U5mANNjgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788cd7f7-7a4b-4b46-affc-ef8d7c6c1b43"
      },
      "source": [
        "pip install -q tf-models-official"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6MB 31.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 15.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2MB 78kB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 53.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 63.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/pyyaml/\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 645kB 48.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 47.0MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWJAQ8--OGbh"
      },
      "source": [
        "import tensorflow_text as text\n",
        "from official.nlp import optimization"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_FUARvkOGfd"
      },
      "source": [
        "preprocessor = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA5AqTjwhNtN"
      },
      "source": [
        "encoder = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb7tFscHhl9D"
      },
      "source": [
        "# input = preprocessor(['Batch of inputs', 'TF Hub makes BERT easy!', 'More text.'])\n",
        "# pooled_output = encoder(input)[\"pooled_output\"]\n",
        "# print(pooled_output[1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZ5RSbqOGi9"
      },
      "source": [
        "# text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "\n",
        "# encoder_inputs = preprocessor(text_input)\n",
        "# encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",trainable=True)\n",
        "# outputs = encoder(encoder_inputs)\n",
        "# pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
        "# sequence_output = outputs[\"sequence_output\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VzgW_J3OGmM"
      },
      "source": [
        "# embedding_model = tf.keras.Model(text_input, pooled_output)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GFzyYfLXvRj"
      },
      "source": [
        "window_size=1080\n",
        "\n",
        "step_size=window_size//6\n",
        "window_time_start=[]\n",
        "window_time_end=[]\n",
        "window_cue=[]\n",
        "\n",
        "#ith denotes  transcript file number\n",
        "\n",
        "for ith in range(50):                \n",
        "    tmp=[]\n",
        "    start=[]\n",
        "    end=[]\n",
        "    for i in range(0, len(words_list[ith]),step_size):\n",
        "        if i + window_size < len(words_list[ith]):\n",
        "            start.append(words_time[ith][i])\n",
        "            end.append(words_time[ith][i+window_size])\n",
        "            strw=\"\"\n",
        "            for w in words_list[ith][i:i+window_size]:\n",
        "                strw+=w+' '\n",
        "            tmp.append(get_continuous_chunks(strw,chunker.parse))\n",
        "    window_time_start.append(start)\n",
        "    window_time_end.append(end)\n",
        "    window_cue.append(tmp)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZMmM2fsOGpY"
      },
      "source": [
        "def calculate_cosine_similarity(vec1,vec2):\n",
        "    csim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "    if np.isnan(np.sum(csim)):\n",
        "        return 0\n",
        "    return csim\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_wfuT5nh8S"
      },
      "source": [
        "      \n",
        "  #vectorize function takes mean of all the words in noun phrases\n",
        "def vectorize(vectors):\n",
        "  wordvec=[]\n",
        "  input=preprocessor(vectors)\n",
        "  pooled_output=encoder(input)[\"pooled_output\"]\n",
        "  for ii in pooled_output:\n",
        "    #print(type(ii))\n",
        "    wordvec.append(ii)\n",
        "  ans=np.mean(wordvec,axis=0)\n",
        "  return ans\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpi7xNKmI5zH"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BEQs8esOGv_",
        "outputId": "39a1bf29-e8fb-4005-abe9-bbe40b7bc117"
      },
      "source": [
        "#score_list contains cosine similarity of ith and (i+1)th window\n",
        "score_list=[]\n",
        "k=0\n",
        "for i in window_cue:\n",
        "    tmp=[]\n",
        "    if k>=15:\n",
        "      break\n",
        "    for j in range(0,len(i)-1):\n",
        "        tmp.append(calculate_cosine_similarity(vectorize(i[j]),vectorize(i[j+1])))\n",
        "    k+=1\n",
        "    print(k)\n",
        "    score_list.append(tmp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoj7JEDDOG13"
      },
      "source": [
        "score_list[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHQRrgt1OG72"
      },
      "source": [
        "path2=r'/content/video-lecture-segmentation/ALV_srt_GT'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbfHDWaMOHBM"
      },
      "source": [
        "#reading all the ground truth files\n",
        "\n",
        "tmp2=[]\n",
        "for i in range(1,101):\n",
        "    tmp2.append(str('%04d'%i))\n",
        "    \n",
        "groundTruth=[open(path2+'/'+i+'.txt',encoding=\"utf8\") for i in tmp2]\n",
        "groundTruth=[i.read() for i in groundTruth]\n",
        "groundTruth=[i.split('\\n') for i in groundTruth]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yu2UTpQOHGP"
      },
      "source": [
        "gtval=[]\n",
        "for i in groundTruth:\n",
        "    tmp=[]\n",
        "    for j in i:\n",
        "        if len(j)>1:\n",
        "            tmp.append(j.split('\\t'))\n",
        "    gtval.append(tmp)\n",
        "gtv=[]\n",
        "for i in gtval:\n",
        "    tmp=[]\n",
        "    for j in i:\n",
        "        if len(j)>1:\n",
        "            tmp.append(j[2])\n",
        "    gtv.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqZSiGdLOHJp"
      },
      "source": [
        "#depth_list contains minimas of score_list\n",
        "#depth_time_list contains time for each depth\n",
        "\n",
        "depth_list=[]\n",
        "depth_time_list=[]\n",
        "z=0\n",
        "for j,k in zip(score_list,window_time_start[:15]):\n",
        "    tmp_list=[]\n",
        "    tmp_time=[]\n",
        "    z+=1\n",
        "    #if(z==6):\n",
        "     # break\n",
        "    for i in range(1,len(j)-1):\n",
        "        if(j[i-1]>j[i] and j[i]<j[i+1]):\n",
        "            tmp_list.append(j[i+1]+j[i-1]-j[i]-j[i])\n",
        "            tmp_time.append(k[i])\n",
        "    depth_list.append(tmp_list)\n",
        "    depth_time_list.append(tmp_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCDb9gXAjirk"
      },
      "source": [
        "len(depth_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0m8KNGdOHQ_"
      },
      "source": [
        "#function diff calculate difference between two time with t1,t2 as timestamp and tsec as permitable difference\n",
        "def diff(t1,t2,tsec):\n",
        "    if len(t1) !=12 or len(t2)!=12:\n",
        "        return False\n",
        "    hh1,mm1,ss1=map(int,t1[0:-4].split(':'))\n",
        "    hh2,mm2,ss2=map(int,t2[0:-4].split(':'))\n",
        "    dh=abs(hh1-hh2)\n",
        "    dm=abs(mm1-mm2)\n",
        "    ds=abs(ss1-ss2)\n",
        "    if dh*60*60+dm*60+ds>=tsec:\n",
        "        return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMk6JPJlOHUN"
      },
      "source": [
        "#sc1,sc2,sc3,sc4 denotes score for 30sec,45sec,60sec and 90 sec permitable difference\n",
        "\n",
        "sc1,sc2,sc3,sc4=[],[],[],[]\n",
        "\n",
        "for i in range(15):\n",
        "    t1,t2,t3,t4=0,0,0,0\n",
        "    s1,s2,s3,s4=set({}),set({}),set({}),set({})\n",
        "    for j in depth_time_list[i]:\n",
        "        for k in range(20):\n",
        "            #print(j,gtv[i][k])\n",
        "            if diff(j,gtv[i][k],30) and gtv[i][k] not in s1:\n",
        "                t1+=1\n",
        "                s1.add(gtv[i][k])\n",
        "            if diff(j,gtv[i][k],45) and gtv[i][k] not in s2:\n",
        "                t2+=1\n",
        "                s2.add(gtv[i][k])\n",
        "            if diff(j,gtv[i][k],60) and gtv[i][k] not in s3:\n",
        "                t3+=1\n",
        "                s3.add(gtv[i][k])\n",
        "            if diff(j,gtv[i][k],90) and gtv[i][k] not in s4:\n",
        "                t4+=1\n",
        "                s4.add(gtv[i][k])\n",
        "    sc1.append(t1)\n",
        "    sc2.append(t2)\n",
        "    sc3.append(t3)\n",
        "    sc4.append(t4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2jT7qvYOHXT"
      },
      "source": [
        "# Precision= score/total_no_of_predicted_boundaries\n",
        "# Recall=  score/actual_number_of_ground_truth_value\n",
        "# F-Score= 2*Precision*Recall/(Precision+Recall)\n",
        "\n",
        "precision1=[sc1[i]/len(depth_time_list[i]) for i in range(15)]\n",
        "precision2=[sc2[i]/len(depth_time_list[i]) for i in range(15)]\n",
        "precision3=[sc3[i]/len(depth_time_list[i]) for i in range(15)]\n",
        "precision4=[sc4[i]/len(depth_time_list[i]) for i in range(15)]\n",
        "recall1=[sc1[i]/20 for i in range(15)]\n",
        "recall2=[sc2[i]/20 for i in range(15)]\n",
        "recall3=[sc3[i]/20 for i in range(15)]\n",
        "recall4=[sc4[i]/20 for i in range(15)]\n",
        "fscore1,fscore2,fscore3,fscore4=[],[],[],[]\n",
        "for i in range(15):\n",
        "    if sc1[i]==0:\n",
        "        fscore1.append(0)\n",
        "    else:\n",
        "        fscore1.append((2*precision1[i]*recall1[i])/(precision1[i]+recall1[i]))\n",
        "    if sc2[i]==0:\n",
        "        fscore2.append(0)\n",
        "    else:\n",
        "        fscore2.append((2*precision2[i]*recall2[i])/(precision2[i]+recall2[i]))\n",
        "    if sc3[i]==0:\n",
        "        fscore3.append(0)\n",
        "    else:\n",
        "        fscore3.append((2*precision3[i]*recall3[i])/(precision3[i]+recall3[i]))\n",
        "    if sc4[i]==0:\n",
        "        fscore4.append(0)\n",
        "    else:\n",
        "        fscore4.append((2*precision4[i]*recall4[i])/(precision4[i]+recall4[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RcoS5o1OHah"
      },
      "source": [
        "# avg_30,avg_45,avg_60,avg_90 average score of 10 files\n",
        "\n",
        "avg_30=sum(fscore1)/15\n",
        "avg_45=sum(fscore2)/15\n",
        "avg_60=sum(fscore3)/15\n",
        "avg_90=sum(fscore4)/15\n",
        "recall_30=sum(recall1)/15\n",
        "recall_45=sum(recall2)/15\n",
        "recall_60=sum(recall3)/15\n",
        "recall_90=sum(recall4)/15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3SFfRnXOHeM"
      },
      "source": [
        "print(avg_30,avg_45,avg_60,avg_90)\n",
        "print(recall_30,recall_45,recall_60,recall_90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IDnp3sOHhL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNjL0FnvOHkB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDA6BbpfOHno"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}